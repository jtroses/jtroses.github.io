[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jtroses.github.io",
    "section": "",
    "text": "Janelle’s Machine Learning Blog\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJanelle Rose\n\n\n\n\n\n\n  \n\n\n\n\nClassification Example\n\n\n\n\n\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nJanelle Rose\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "The plot below is from the Seaborn Python Library documentation\n\n5 blog posts, not 3\nTopics are pre-defined\n\nProbability theory and random variables\n\nHistogram\n\nClustering\n\nKmeans clustering\nDBSCAN labels for scatter plot\n\nLinear and nonlinear regression\n\nline or scatter plot, Random Forest\n\nClassification\n\nROC, PR, Confusion Matrix, Random Forest\nK nearest neighbor\n\nAnomaly/outlier detection\n\nDBSCAN labels for scatter plot\n\n\n\nLearing objectoves: 1. Use various techniques related to preprocessing prior to the use of machine learning models. 2. Describe the probability theory and random variables. 3. Identify the common tasks in machine learning/data mining models for clustering. 4. Analyze multiple linear and nonlinear regression. 5. Describe the algorithms, theories, and applications related to machine learning/data mining for classification. 6. Detect anomaly/outlier behavior and the treatment techniques.\n\n\n\n\n\nThe plot below is from the Yellowbrick Python Library documentation\n\nfrom yellowbrick.datasets import load_concrete\nfrom yellowbrick.features import JointPlotVisualizer\n\n# Load the dataset\nX, y = load_concrete()\n\n# Instantiate the visualizer\nvisualizer = JointPlotVisualizer(columns=\"cement\")\n\nvisualizer.fit_transform(X, y)        # Fit and transform the data\nvisualizer.show()     \n\n\n\n\n&lt;Axes: xlabel='cement', ylabel='target'&gt;"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I created this Blog for my Intro to machine learning course while I am learning different machine learning topics. Hopefully this examples can help someone work through some introductory topics"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Classification Example",
    "section": "",
    "text": "Classification is a supervised machine learning technique. Supervised training sets have desired solutions called labels. For the example below we are going to use classification to try and predict whether or not the an individual is in a senior leadership role.\nInformation about the dataset can be found here: salary-by-job-title-and-country\n\nRead in Dataset\n\nimport pandas as pd\nsalary = pd.read_csv('../../data/Salary.csv')\nsalary.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6684 entries, 0 to 6683\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   Age                  6684 non-null   float64\n 1   Gender               6684 non-null   object \n 2   Education Level      6684 non-null   int64  \n 3   Job Title            6684 non-null   object \n 4   Years of Experience  6684 non-null   float64\n 5   Salary               6684 non-null   float64\n 6   Country              6684 non-null   object \n 7   Race                 6684 non-null   object \n 8   Senior               6684 non-null   int64  \ndtypes: float64(3), int64(2), object(4)\nmemory usage: 470.1+ KB\n\n\n\n\nData Transformation\nLet’s visualize the numeric variables with histograms to check the shape of the data.\n\nimport matplotlib.pyplot as plt\n\nsalary.hist(bins = 20, figsize=(12, 8))\nplt.show()\n\n\n\n\nGender is a categorical attribute that is part of the model. Let’s convert the categorical variable to a dummy variable that we can use in our classification model.\n\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(sparse_output = False).set_output(transform='pandas')\ngender_oh = enc.fit_transform(salary[['Gender']])\nsalary = pd.concat([salary,gender_oh],axis=1)\nsalary\n\n\n\n\n\n\n\n\nAge\nGender\nEducation Level\nJob Title\nYears of Experience\nSalary\nCountry\nRace\nSenior\nGender_Female\nGender_Male\n\n\n\n\n0\n32.0\nMale\n1\nSoftware Engineer\n5.0\n90000.0\nUK\nWhite\n0\n0.0\n1.0\n\n\n1\n28.0\nFemale\n2\nData Analyst\n3.0\n65000.0\nUSA\nHispanic\n0\n1.0\n0.0\n\n\n2\n45.0\nMale\n3\nManager\n15.0\n150000.0\nCanada\nWhite\n1\n0.0\n1.0\n\n\n3\n36.0\nFemale\n1\nSales Associate\n7.0\n60000.0\nUSA\nHispanic\n0\n1.0\n0.0\n\n\n4\n52.0\nMale\n2\nDirector\n20.0\n200000.0\nUSA\nAsian\n0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6679\n49.0\nFemale\n3\nDirector of Marketing\n20.0\n200000.0\nUK\nMixed\n0\n1.0\n0.0\n\n\n6680\n32.0\nMale\n0\nSales Associate\n3.0\n50000.0\nAustralia\nAustralian\n0\n0.0\n1.0\n\n\n6681\n30.0\nFemale\n1\nFinancial Manager\n4.0\n55000.0\nChina\nChinese\n0\n1.0\n0.0\n\n\n6682\n46.0\nMale\n2\nMarketing Manager\n14.0\n140000.0\nChina\nKorean\n0\n0.0\n1.0\n\n\n6683\n26.0\nFemale\n0\nSales Executive\n1.0\n35000.0\nCanada\nBlack\n0\n1.0\n0.0\n\n\n\n\n6684 rows × 11 columns\n\n\n\nThe target attribute that we want to classify is ‘Senior’. The ‘Senior’ attribute is currently an integer we are going to change it to a boolean for the purpose of this model. So ‘1’ indicates that an individual has a senior position and ‘0’ means that an individual does not has a senior position.\n\nsalary['senior_bool'] = salary.Senior.astype('bool')\nsalary\n\n\n\n\n\n\n\n\nAge\nGender\nEducation Level\nJob Title\nYears of Experience\nSalary\nCountry\nRace\nSenior\nGender_Female\nGender_Male\nsenior_bool\n\n\n\n\n0\n32.0\nMale\n1\nSoftware Engineer\n5.0\n90000.0\nUK\nWhite\n0\n0.0\n1.0\nFalse\n\n\n1\n28.0\nFemale\n2\nData Analyst\n3.0\n65000.0\nUSA\nHispanic\n0\n1.0\n0.0\nFalse\n\n\n2\n45.0\nMale\n3\nManager\n15.0\n150000.0\nCanada\nWhite\n1\n0.0\n1.0\nTrue\n\n\n3\n36.0\nFemale\n1\nSales Associate\n7.0\n60000.0\nUSA\nHispanic\n0\n1.0\n0.0\nFalse\n\n\n4\n52.0\nMale\n2\nDirector\n20.0\n200000.0\nUSA\nAsian\n0\n0.0\n1.0\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6679\n49.0\nFemale\n3\nDirector of Marketing\n20.0\n200000.0\nUK\nMixed\n0\n1.0\n0.0\nFalse\n\n\n6680\n32.0\nMale\n0\nSales Associate\n3.0\n50000.0\nAustralia\nAustralian\n0\n0.0\n1.0\nFalse\n\n\n6681\n30.0\nFemale\n1\nFinancial Manager\n4.0\n55000.0\nChina\nChinese\n0\n1.0\n0.0\nFalse\n\n\n6682\n46.0\nMale\n2\nMarketing Manager\n14.0\n140000.0\nChina\nKorean\n0\n0.0\n1.0\nFalse\n\n\n6683\n26.0\nFemale\n0\nSales Executive\n1.0\n35000.0\nCanada\nBlack\n0\n1.0\n0.0\nFalse\n\n\n\n\n6684 rows × 12 columns\n\n\n\n\n\nCorrelation Matrix\nLet’s look at the correlation between ‘senior_bool’ and the numerical variables in the salary dataset.\n\ncorr_matrix = salary.corr()\ncorr_matrix['senior_bool'].sort_values(ascending = False)\n\nC:\\Users\\janel\\AppData\\Local\\Temp\\ipykernel_13632\\2222679277.py:1: FutureWarning:\n\nThe default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\nSenior                 1.000000\nsenior_bool            1.000000\nAge                    0.334070\nYears of Experience    0.317877\nEducation Level        0.273466\nSalary                 0.223636\nGender_Male            0.038852\nGender_Female         -0.038852\nName: senior_bool, dtype: float64\n\n\nThere does not seem to be a very high correlation with the senior_bool attribute but we’ll see how good of a classification model we can build.\n\n\nCreate Train Test Split Datasets\n\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(salary, test_size=0.2, random_state = 35)\n\nThe dimensions of the training set:\n\ntrain_set.shape\n\n(5347, 12)\n\n\nThe dimentions of the test set:\n\ntest_set.shape\n\n(1337, 12)\n\n\nTraining variables\n\nX_train = train_set[['Age', 'Salary', 'Years of Experience', 'Education Level']].to_numpy()\ny_train = train_set['senior_bool'].apply(str).to_numpy()\n\nTest variables\n\nX_test = test_set[['Age', 'Salary', 'Years of Experience', 'Education Level']].to_numpy()\ny_test = test_set['senior_bool'].apply(str).to_numpy()\n\n\n\nStochastic Gradient Descent Classifier\nFirst we will try the Stochastic gradient classifier to see how well it does at predicting whether a person has a senior role or not.\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_score\nsgd_clf = SGDClassifier(random_state=42)\ncross_val_score(sgd_clf, X_train, y_train, cv = 5, scoring='accuracy')\n\narray([0.14672897, 0.85327103, 0.85313377, 0.85313377, 0.85313377])\n\n\nIn 4 out of 5 of the folds the model scores over 85% but the 14% fold is pretty bad. Let’s compare it to the dummy classifier, which always selects the most frequent classifier which is not a senior.\n\n\nDummy Classifier\n\nfrom sklearn.dummy import DummyClassifier\ndummy_clf = DummyClassifier(random_state = 42)\ncross_val_score(dummy_clf, X_train, y_train, cv = 5, scoring='accuracy')\n\narray([0.85327103, 0.85327103, 0.85313377, 0.85313377, 0.85313377])\n\n\nIt looks like the dummy classifier performs better than the SGD classifier. Let see if we can improve upon the performance with the Random Forest Classifier.\n\n\nRandom Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(random_state=42)\ncross_val_score(forest_clf, X_train, y_train, cv = 5, scoring='accuracy')\n\narray([0.95607477, 0.96261682, 0.94574369, 0.95416277, 0.95509822])\n\n\nThe random forest classifier performs better than both the SGD and Dummy classifiers. CPrrectly predicting whether or not a person has a senior role of not over 94% of the time.\nSome of our attributes distributions were a bit skewed, let’s see if applying the Standard scaler improves the performance of the model.\n\n\nApplying Standard Scaler to the Training Attributes\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype('float64'))\ncross_val_score(forest_clf, X_train_scaled, y_train, cv = 5, scoring='accuracy')\n\narray([0.95607477, 0.96261682, 0.94574369, 0.95416277, 0.95603368])\n\n\nThe scaler does not really improve the model, so we will stick with the raw data and create a Confusion Matri.\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(forest_clf, X_train, y_train, cv = 5)\ny_train_pred\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, normalize = \"true\", values_format = \".0%\")\nplt.show()\n\n\n\n\nThe confusion matrix shows us that 97% of the time there is a true Negative i.e a person who is identified as not being in a senior role is acurately categorized as false 97% of the time. On the other side of chart, 84% of the time there is a True Positive i.e. a person who is identifies as having a senior role is correct 84% of the time.\n\n\nEvaluate Test Set\n\nforest_clf.fit(X_train, y_train) # fit the Random Forest model \ny_test_predict = forest_clf.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, y_test_predict, normalize = \"true\", values_format = '.0%')\nplt.show()\n\n\n\n\nFor evaluating the test set, 96% of the time there is a true Negative i.e a person who is identified as not being in a senior role is acurately categorized as false 96% of the time. On the other side of chart, 85% of the time there is a True Positive i.e. a person who is identifies as having a senior role is correct 85% of the time. Which aligns closely with what we saw in the training dataset."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "jtroses.github.io",
    "section": "",
    "text": "Janelle’s Machine Learning Blog"
  },
  {
    "objectID": "docs/posts/post-with-code/index.html",
    "href": "docs/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "Janelle Rose\n2023-10-14\nThe plot below is from the Seaborn Python Library documentation"
  },
  {
    "objectID": "posts/welcome/index.html#check-test-train-eval-split",
    "href": "posts/welcome/index.html#check-test-train-eval-split",
    "title": "Classification Example",
    "section": "Check test, train, eval split",
    "text": "Check test, train, eval split"
  }
]